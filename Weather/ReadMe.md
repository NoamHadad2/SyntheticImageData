# Autonomous Driving Small-Object Detection with Synthetic Weather Augmentation

## Abstract

This project investigates whether **synthetic weather augmentation** can improve object detection robustness for autonomous driving systems. We generate synthetic rain, snow, and fog variants of real driving images while preserving ground-truth object annotations, then evaluate the impact on YOLO-based detection models. The focus is on safety-critical small objects (pedestrians, cyclists) that are particularly challenging under adverse weather conditions.

---

## 1. Problem Statement

Autonomous vehicles must reliably detect objects in all weather conditions. However:
- Real adverse-weather datasets are scarce and expensive to collect
- Small objects (pedestrians, cyclists) are especially vulnerable to weather-induced degradation
- Models trained on clear-weather data often fail in rain, snow, or fog

**Hypothesis**: Augmenting training data with realistic synthetic weather variants will improve model robustness without requiring additional real-world data collection.

---

## 2. Dataset

| Metric | Value |
|--------|-------|
| **Source** | KITTI-format autonomous driving dataset |
| **Total Images** | 14,962 (7,481 train / 7,481 val) |
| **Total Objects** | 103,730 annotated bounding boxes |
| **Annotation Format** | KITTI (15-column text files) |
| **Image Format** | PNG (1242×375 typical resolution) |

### Class Distribution

| Class | Count | Percentage |
|-------|-------|------------|
| Car | 57,484 | 55.4% |
| DontCare | 22,590 | 21.8% |
| Pedestrian | 8,974 | 8.7% |
| Van | 5,828 | 5.6% |
| Cyclist | 3,254 | 3.1% |
| Truck | 2,188 | 2.1% |
| Misc | 1,946 | 1.9% |
| Tram | 1,022 | 1.0% |
| Person_sitting | 444 | 0.4% |

---

## 3. Exploratory Data Analysis Findings

Key insights from our analysis:

1. **Class Imbalance**: 129.5× ratio between most common (Car) and rarest (Person_sitting) classes
   - *Implication*: May need class weighting during training

2. **Small Objects Prevalent**: 56.5% of objects occupy <1% of image area
   - *Implication*: Higher input resolution may improve detection

3. **Occlusion Challenges**: 25.4% heavily occluded, 7.5% significantly truncated
   - *Implication*: Synthetic weather must preserve already-difficult objects

4. **Diverse Aspect Ratios**: Cars are typically wide, pedestrians are typically tall
   - *Implication*: Anchor boxes should cover diverse ratios

### Sample Visualizations

The following figures are generated by `01_eda.ipynb` and saved in `results/figures/`:

| Figure | Description |
|--------|-------------|
| `class_distribution.png` | Bar chart of object class frequencies |
| `bbox_area_distribution.png` | Histogram of bounding box areas |
| `sample_gt_visualization.png` | Random images with ground-truth boxes |

---

## 4. Synthetic Data Generation

### 4.1 Method Overview

We apply **OpenCV-based weather simulation** to generate synthetic variants of training images:

- **Weather Types**: Rain, Snow, Fog
- **Intensity Level**: Extreme (for maximum augmentation effect)
- **Object Preservation**: SSIM-based scoring ensures objects remain recognizable

### 4.2 Generation Results

| Metric | Value |
|--------|-------|
| **Synthetic Images Generated** | 2,696 |
| **Rain Images** | 865 (32.1%) |
| **Snow Images** | 922 (34.2%) |
| **Fog Images** | 909 (33.7%) |
| **Mean Preservation Score** | 0.91 (SSIM-based) |
| **Preservation Score Range** | 0.47 – 1.00 |
| **Manifest File** | `data/synthetic/manifest.csv` |

---

### 4.3 Synthetic Weather Pipeline (Step-by-Step)

The generation pipeline implemented in `notebooks/02_synthetic_generation.ipynb` follows these steps:

#### Step 1: Input Selection
```
Source:     data/images/train/*.png
Count:      Training set images (subset processed based on PREVIEW mode)
Split:      Only training images are augmented; validation images remain unchanged
```

Image-label pairs are loaded from `results/metrics/image_label_pairs.pkl` (generated by `00_setup.ipynb`).

#### Step 2: Annotation Handling
- **KITTI labels are parsed** using a 15-column format (class, truncation, occlusion, alpha, bbox, ...)
- **Objects with class `DontCare` are filtered out** during mask generation and quality scoring
- **Labels are copied unchanged** to `data/synthetic/labels/` with matching filenames
- Bounding boxes remain identical between real and synthetic images

#### Step 3: Object Mask Generation
Two methods are supported (with automatic fallback):

| Method | When Used | Description |
|--------|-----------|-------------|
| **SAM ViT-H** | GPU available + model loaded | Precise instance segmentation per object |
| **Bounding Box Mask** | Fallback (no GPU) | Rectangular masks with 10px dilation |

```python
# Bounding box fallback creates rectangular masks:
mask[y1-10 : y2+10, x1-10 : x2+10] = 1.0
```

#### Step 4: Weather Effect Application

Weather effects are applied using **OpenCV-based procedural methods**:

| Weather | Operations | Key Parameters |
|---------|------------|----------------|
| **Rain** | Brightness reduction + vertical streak overlay | `intensity=0.8`, ~400 rain drops, length 10–30px |
| **Fog** | Vertical gradient alpha blend with fog color | `intensity=0.8`, RGB(220,220,230), density 0.16–0.64 |
| **Snow** | Brightness shift + white circle overlay | `intensity=0.8`, ~800 flakes, radius 1–4px |

**Implementation Details:**

```python
# Rain: Darken image + add vertical streaks
result *= (0.6 + 0.2 * (1 - intensity))   # Reduce brightness
for _ in range(int(500 * intensity)):      # Add rain drops
    cv2.line(result, (x, y), (x, y+length), (200, 200, 220), 1)

# Fog: Linear gradient from top to bottom
fog_density = np.linspace(0.2 * intensity, 0.8 * intensity, height)
result = result * (1 - fog_density) + fog_color * fog_density

# Snow: Brighten image + add white circles
result = result * 0.9 + 30               # Brighten
for _ in range(int(1000 * intensity)):   # Add snowflakes
    cv2.circle(result, (x, y), radius, (255, 255, 255), -1)
```

**Object Preservation Strategy:**
When SAM/SDXL is unavailable, objects are preserved using mask-based compositing:
```python
mask_3ch = object_mask[:, :, np.newaxis]
result = (image * mask_3ch + weather_img * (1 - mask_3ch)).astype(np.uint8)
```

#### Step 5: Quality Scoring (SSIM-Based)

The preservation score measures how well objects are retained after weather application:

```python
def compute_preservation_score(original, synthetic, objects):
    for obj in objects:
        if obj['class_name'] == 'DontCare': continue
        
        # Extract grayscale crops at object bounding box
        orig_crop = cv2.cvtColor(original[y1:y2, x1:x2], cv2.COLOR_RGB2GRAY)
        synth_crop = cv2.cvtColor(synthetic[y1:y2, x1:x2], cv2.COLOR_RGB2GRAY)
        
        # Compute SSIM (default win_size from skimage)
        score = ssim(orig_crop, synth_crop)
        scores.append(score)
    
    return np.mean(scores), passed_ratio
```

| Parameter | Value |
|-----------|-------|
| **Color Space** | Grayscale (RGB→GRAY conversion) |
| **SSIM Threshold** | 0.85 |
| **Min Objects Pass Ratio** | 0.80 (80% of objects must exceed threshold) |
| **Minimum Crop Size** | 7×7 pixels (smaller objects skipped) |

**Quality Control Logic:**
- Generate `K` candidates per image (currently `K=1`)
- Select candidate with highest preservation score if ≥80% objects pass threshold
- If no candidate meets threshold, use the first generated image as fallback

#### Step 6: Output Writing

**Folder Structure:**
```
data/synthetic/
├── images/          # 2,696 synthetic PNG images
│   └── {id}_{weather}_{intensity}_synth.png
├── labels/          # 2,696 copied KITTI label files
│   └── {id}_{weather}_{intensity}_synth.txt
└── manifest.csv     # Generation metadata
```

**Filename Convention:**
```
{original_id}_{weather_type}_{intensity}_synth.{ext}

Examples:
  000042_rain_extreme_synth.png
  001337_fog_extreme_synth.png
  002500_snow_extreme_synth.txt
```

**Manifest CSV Columns:**
| Column | Description |
|--------|-------------|
| `original_path` | Path to source image (e.g., `data/images/train/000042.png`) |
| `synthetic_path` | Path to generated image |
| `weather_type` | `rain`, `fog`, or `snow` |
| `intensity_level` | `extreme` (only level used) |
| `seed` | Random seed used for generation |
| `preservation_score` | SSIM-based quality metric (0.0–1.0) |

---

### 4.4 Weather Effect Details

| Weather Type | Visual Effect | Key Transformations |
|--------------|---------------|---------------------|
| **Rain** | Vertical streaks, darker scene | Brightness ×0.68, 400 streaks (10–30px), slight blue tint |
| **Fog** | Atmospheric haze, reduced contrast | Gradient alpha blend (16%–64%), gray overlay RGB(220,220,230) |
| **Snow** | White particles, slightly brighter | Brightness ×0.9+30, 800 circular flakes (r=1–4px) |

---

### 4.5 Preservation Score Distribution

Quality metrics from the generated synthetic dataset:

| Statistic | Value |
|-----------|-------|
| **Count** | 2,696 images |
| **Mean** | 0.912 |
| **Std Dev** | 0.080 |
| **Min** | 0.471 |
| **25th Percentile** | 0.890 |
| **Median** | 0.935 |
| **75th Percentile** | 0.963 |
| **Max** | 1.000 |

> Most generated images achieve preservation scores >0.85, indicating objects remain well-preserved after weather effects are applied.

---

### 4.6 Real vs Synthetic Examples

To visualize the generation results, you can compare real images with their synthetic counterparts:

**Example Pairs** (from `data/images/train/` → `data/synthetic/images/`):

| Real Image | Synthetic Image | Weather | Score |
|------------|-----------------|---------|-------|
| `002040.png` | `002040_fog_extreme_synth.png` | Fog | 0.943 |
| `002042.png` | `002042_rain_extreme_synth.png` | Rain | 0.845 |
| `000000.png` | `000000_snow_extreme_synth.png` | Snow | — |

**To export comparison images**, run the following in a notebook:
```python
import cv2
import matplotlib.pyplot as plt
import os

# Create assets directory
os.makedirs('assets', exist_ok=True)

# Example pairs
examples = [
    ('data/images/train/002040.png', 'data/synthetic/images/002040_fog_extreme_synth.png', 'Fog'),
    ('data/images/train/000006.png', 'data/synthetic/images/000006_rain_extreme_synth.png', 'Rain'),
    ('data/images/train/000000.png', 'data/synthetic/images/000000_snow_extreme_synth.png', 'Snow'),
]

fig, axes = plt.subplots(3, 2, figsize=(14, 12))
for i, (real_path, synth_path, weather) in enumerate(examples):
    real = cv2.cvtColor(cv2.imread(real_path), cv2.COLOR_BGR2RGB)
    synth = cv2.cvtColor(cv2.imread(synth_path), cv2.COLOR_BGR2RGB)
    
    axes[i, 0].imshow(real)
    axes[i, 0].set_title(f'Real: {os.path.basename(real_path)}')
    axes[i, 0].axis('off')
    
    axes[i, 1].imshow(synth)
    axes[i, 1].set_title(f'Synthetic ({weather})')
    axes[i, 1].axis('off')

plt.tight_layout()
plt.savefig('assets/real_vs_synthetic_comparison.png', dpi=150)
plt.show()
```

**Typical Artifacts Observed:**
- **Rain**: Visible vertical streaks may occasionally overlap small objects
- **Fog**: Gradient can cause uneven visibility (clearer at bottom, hazier at top)
- **Snow**: White particles may blend with light-colored objects

---

### 4.7 Reproducibility

**To regenerate the synthetic dataset:**

1. Ensure `00_setup.ipynb` has been run (creates `image_label_pairs.pkl`)
2. Open `notebooks/02_synthetic_generation.ipynb`
3. Configure settings in the first cell:
   ```python
   SEED = 42              # Deterministic generation
   PREVIEW = False        # Full generation mode
   PREVIEW_SAMPLES = 5    # Number of samples if PREVIEW=True
   ```
4. Run all cells

**PREVIEW Mode:**
- Set `PREVIEW = True` to process only 5 images for quick testing
- Useful for validating the pipeline before full generation


**Random Seed Control:**
- `SEED = 42` ensures reproducible random choices (weather type, drop positions)
- Each candidate uses `seed = SEED + i * 100` for variation in multi-candidate mode

---

## 5. Project Structure

```
Autonomous_Driving_Project/
├── notebooks/
│   ├── 00_setup.ipynb           # Environment setup + data validation
│   ├── 01_eda.ipynb             # Exploratory data analysis
│   ├── 02_synthetic_generation.ipynb  # Weather effect generation
│   └── 03_train_eval_compare.ipynb    # Model training + evaluation
├── data/
│   ├── images/
│   │   ├── train/               # 7,481 training images
│   │   └── val/                 # 7,481 validation images
│   ├── labels/                  # KITTI-format annotations
│   └── synthetic/
│       ├── images/              # 2,696 synthetic images
│       ├── labels/              # Copied annotations
│       └── manifest.csv         # Generation metadata
├── results/
│   ├── figures/                 # EDA visualizations
│   └── metrics/                 # Analysis reports (JSON, CSV)
└── README.md
```

---

## 6. How to Run

### Prerequisites
- Google Colab (recommended) or local Python 3.8+ environment
- GPU recommended for synthetic generation and training

### Step-by-Step Execution

#### 1. Upload to Google Drive
```
Upload the entire project folder to:
/content/drive/MyDrive/Autonomous_Driving_Project/
```

#### 2. Run Notebooks in Order

| Notebook | Purpose | Estimated Time |
|----------|---------|----------------|
| `00_setup.ipynb` | Install dependencies, validate data 
| `01_eda.ipynb` | Generate analysis plots and statistics 
| `02_synthetic_generation.ipynb` | Generate weather variants 
| `03_train_eval_compare.ipynb` | Train and compare models 


#### 3. Key Configuration
```python
# In each notebook, set the project root:
PROJECT_ROOT = "/content/drive/MyDrive/Autonomous_Driving_Project"

# For quick testing, enable PREVIEW mode in 02_synthetic_generation.ipynb:
PREVIEW = True  # Process only 5 images
```

---

## 7. Current Status & Limitations

> [!IMPORTANT]
> **This project is a work in progress. Final training results are not yet available.**

### Completed
- ✅ Dataset loading and validation
- ✅ Comprehensive exploratory data analysis
- ✅ Synthetic weather generation (2,696 images)
- ✅ Pipeline infrastructure for training

### Pending
- ⏳ Model training (baseline vs. mixed data)
- ⏳ Performance comparison and metrics
- ⏳ Test set evaluation

### Known Limitations

1. **No Final Model Results**: Training has not been executed; mAP comparisons are not available
2. **Simplified Weather Effects**: Current implementation uses OpenCV-based simulation rather than advanced AI methods (SAM, SDXL, ControlNet)
3. **Resource Requirements**: Full training may require Colab Pro or local GPU
4. **Class Imbalance**: No class weighting implemented yet

---

## 8. Next Steps

1. **Execute Training**: Run `03_train_eval_compare.ipynb` to train baseline and mixed models
2. **Compare Performance**: Evaluate mAP@0.5, mAP@0.5:0.95 on test set
3. **Analyze Per-Class Impact**: Focus on small object classes (Pedestrian, Cyclist)
4. **Iterate on Synthetic Quality**: Consider advanced generation methods if needed

---

## 9. Dependencies

```
# Core
torch>=2.0.0
torchvision
ultralytics  # YOLOv8
opencv-python
numpy
pandas
matplotlib
seaborn
pillow
tqdm
scikit-image  # For SSIM computation

# Optional (for advanced generation)
diffusers
transformers
segment-anything
```

---

## 10. References

- KITTI Dataset: [www.cvlibs.net/datasets/kitti](http://www.cvlibs.net/datasets/kitti/)
- YOLOv8: [docs.ultralytics.com](https://docs.ultralytics.com/)
- Synthetic Weather Augmentation: Domain adaptation techniques for autonomous driving


